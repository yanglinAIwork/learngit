### 一、内存及CPU

​	使用Top命令

1、top查询当前服务器内存信息

~~~
top
~~~

展示结果：

~~~
top - 09:40:14 up 434 days, 42 min,  1 user,  load average: 0.25, 0.14, 0.10
Mem:   3924388k total,  3807952k used,   116436k free,     4352k buffers
Swap:  4194300k total,  1899948k used,  2294352k free,   187048k cached

  PID USER      PR  NI  VIRT  RES  SHR S %CPU %MEM    TIME+  COMMAND
19913 slupuser  20   0 3637m 579m 5092 S 11.7 15.1  39:01.49 java
 1209 slupuser  20   0 4388m 494m 4672 S  4.7 12.9   1447:28 java
  764 slupuser  20   0 2989m 280m 2880 S  2.3  7.3  35:59.94 java
 9574 slupuser  20   0 15032 1196  908 R  2.3  0.0   0:00.02 top
15838 slupuser  20   0 3675m 727m 6052 S  2.3 19.0  34:11.94 java
31813 consul    20   0 46836  20m 3616 S  2.3  0.5 391:48.39 consul
    1 root      20   0 19360  360  180 S  0.0  0.0   0:04.47 init
    2 root      20   0     0    0    0 S  0.0  0.0   0:00.03 kthreadd
    3 root      RT   0     0    0    0 S  0.0  0.0  11:51.97 migration/0
~~~

2、查看该PID详细内存信息

~~~
top -Hp $PID
~~~

展示结果：

~~~
-bash-4.1$ top -Hp 19913
top - 09:40:40 up 434 days, 42 min,  1 user,  load average: 0.38, 0.18, 0.11
Tasks: 173 total,   1 running, 172 sleeping,   0 stopped,   0 zombie
Cpu(s):  9.7%us,  3.7%sy,  0.0%ni, 44.6%id, 41.6%wa,  0.0%hi,  0.5%si,  0.0%st
Mem:   3924388k total,  3807136k used,   117252k free,     1284k buffers
Swap:  4194300k total,  1899968k used,  2294332k free,   127712k cached

  PID USER      PR  NI  VIRT  RES  SHR S %CPU %MEM    TIME+  COMMAND
20356 slupuser  20   0 3637m 647m 4732 S  2.3 16.9   0:11.88 java
20380 slupuser  20   0 3637m 647m 4732 S  2.3 16.9   0:08.39 java
20362 slupuser  20   0 3637m 647m 4732 S  2.0 16.9   0:12.05 java
20384 slupuser  20   0 3637m 647m 4732 S  2.0 16.9   0:12.30 java
20385 slupuser  20   0 3637m 647m 4732 S  1.7 16.9   0:09.11 java
20370 slupuser  20   0 3637m 647m 4732 R  1.3 16.9   0:09.99 java
19919 slupuser  20   0 3637m 647m 4732 S  0.3 16.9   0:28.05 java
19926 slupuser  20   0 3637m 647m 4732 S  0.3 16.9   1:48.25 java
20327 slupuser  20   0 3637m 647m 4732 S  0.3 16.9   4:25.83 java
20462 slupuser  20   0 3637m 647m 4732 S  0.3 16.9   0:02.36 java
~~~

3、将需要查看的PID转为16进制

~~~
printf "%x\n" 20356
~~~

输出结果：

~~~
4f84
~~~

4、查看当前java进程的堆栈状态

~~~
jstat -gcutil
~~~

~~~
jstat -gcutil 9 1000 20356
~~~

展示结果：

| S0   | S1   | E    | O     | M     | CCS   | YGC  | YGCT  | FGC  | FGCT  | GCT   |
| ---- | ---- | ---- | ----- | ----- | ----- | ---- | ----- | ---- | ----- | ----- |
| 0.00 | 0.00 | 0.00 | 75.07 | 59.09 | 59.60 | 3259 | 0.919 | 6517 | 7.715 | 8.635 |
| 0.00 | 0.00 | 0.00 | 0.08  | 59.09 | 59.60 | 3306 | 0.930 | 6611 | 7.822 | 8.752 |
| 0.00 | 0.00 | 0.00 | 0.08  | 59.09 | 59.60 | 3351 | 0.943 | 6701 | 7.924 | 8.867 |
| 0.00 | 0.00 | 0.00 | 0.08  | 59.09 | 59.60 | 3397 | 0.955 | 6793 | 8.029 | 8.984 |

​	可以看到，这里FGC指的是Full GC数量，这里高达6793，而且还在不断增长。从而进一步证实了是由于内存溢出导致的系统缓慢。那么这里确认了内存溢出，但是如何查看你是哪些对象导致的内存溢出呢，这个可以dump出内存日志，然后通过eclipse的mat工具进行查看，如下是其展示的一个对象树结构：

![1558662840871](F:\GitDepository\学习文档\Linux常用命令\images\Eclipse-mat.png)

​	经过mat工具分析之后，我们基本上就能确定内存中主要是哪个对象比较消耗内存，然后找到该对象的创建位置，进行处理即可。这里的主要是PrintStream最多，但是我们也可以看到，其内存消耗量只有12.2%。也就是说，其还不足以导致大量的Full GC，此时我们需要考虑另外一种情况，就是代码或者第三方依赖的包中有显示的 `System.gc()`调用。这种情况我们查看dump内存得到的文件即可判断，因为其会打印GC原因：

![1558662967343](F:\GitDepository\学习文档\Linux常用命令\images\dump.png)

​	比如这里第一次GC是由于 `System.gc()`的显示调用导致的，而第二次GC则是JVM主动发起的。总结来说，对于Full GC次数过多，主要有以下两种原因：

- 代码中一次获取了大量的对象，导致内存溢出，此时可以通过eclipse的mat工具查看内存中有哪些对象比较多；
- 内存占用不高，但是Full GC次数还是比较多，此时可能是显示的 `System.gc()`调用导致GC次数过多，这可以通过添加 `-XX:+DisableExplicitGC`来禁用JVM对显示GC的响应。

---

### 二、端口

#### 1、查看被占用的端口

​	命令格式：**netstat -tunpl**

​	查看所有被占用的端口及被哪个pid进程占用

| 参数 | 解释                                   |
| ---- | -------------------------------------- |
| -t   | 仅显示和tcp相关的                      |
| -u   | 仅显示和udp相关的                      |
| -n   | 不显示别名，能显示数字的全部转换为数字 |
| -l   | 仅显示出于Listen（监听）状态的         |
| -p   | 显示建立这些链接的程序名               |

#### 2、查看具体端口占用

​	命令格式：**lsof -i:PORT**

​	显示格式：

| 进程的名称 | 进程PID | 进程所有者 | 文件描述符 | 协议类型 | 端口号    | 偏移     | 协议名 | 节点名           |
| ---------- | ------- | ---------- | ---------- | -------- | --------- | -------- | ------ | ---------------- |
| COMMAND    | PID     | USER       | FD         | TYPE     | DEVICE    | SIZE/OFF | NODE   | NAME             |
| java       | 9956    | appuser    | 22u        | IPv6     | 565202053 | 0t0      | TCP    | *:11200 (LISTEN) |

#### 3、测试端口连通性

​	四种常用的方法

​	1、telnet 方法

​	格式：**telnet IP PORT**

​	2、wget 方法

​	格式：**wget IP:PORT**

​		**wget IP:PORT/路径**

​	3、ssh 方法

​	格式：**ssh userName@IP**

​	4、curl 方法

​	格式：**curl IP:PORT/路径**

---

### 三、Linux查看磁盘空间

#### 1、查看磁盘剩余空间

命令格式：df -hl

显示格式： 

| 文件系统                      | 容量 | 已用 | 可用  | 已用% | 挂载点     |
| ----------------------------- | ---- | ---- | ----- | ----- | ---------- |
| Filesystem                    | Size | Used | Avail | Use%  | Mounted on |
| /dev/mapper/VolGroup00-lv_app | 20G  | 1.5G | 18G   | 9%    | /app       |

---

#### 2、查看具体空间

**（1）、返回该目录的大小**

命令格式：**du -sh 目录名**

**（2）、查看指定文件夹下所有文件的大小**

命令格式：**du -h 目录名**

---

### 四、Linux删除大文件数据的方法

#### 1、rm命令

​	如果用rm删除几十G的大文件，这里有巨坑！！！

​	因为被删除的文件在删除的时侯还是进程在操作(打开、访问等)的缘故，rm只完成了在磁盘上文件实体的释放，而类似free list结构中相应的文件系统因进程的操作相应的inode并未释放。

​	**解决的方法：**这样的问题解决起来也很简单，找到操作的进程，kill掉就可以了，可是找到操作的进程恰恰是本问题的难点和关键。这样的问题也可以通过重启机器和nmount/mount文件系统这样的方式解决，但这样的方法我是不提倡的，小小的问题就重启机器，小题大做。	

​	linux及solaris可以这样做: 
​	a、下载一个lsof软件装上，google上可以搜到 
​	b、找到正在用被删文件的进程 
​	lsof | grep deleted 

​	c、kill掉相应的进程空间就释放了

---

#### 2、重定向

​	通过 shell 重定向 null （不存在的事物）到该文件：

> ~~~
> # > file.log
> ~~~
>
> ---
>
> #### 3、使用rsync命令
>
> ​	假如你有一些特别大的文件要删除，比如nohup.out这样的实时更新的文件，动辄都是几十个G上百G的，也可以用rsync来清空大文件，而且效率比较高。
>
> ​	1）创建空文件
>
> ​	\# touch/data/blank.txt
>
> ​	2）用rsync清空文件
>
> ​	\# rsync -a --delete-before --progress --stats /root/blank.txt /root/nohup.out
>
> ​	**快速删除大量文件**
>
> ​	假如你要在linux下删除大量文件，比如100万、1000万，像/var/spool/clientmqueue/的mail邮件，/usr/local/nginx/proxy_temp的nginx缓存等，那么rm -rf *可能就不好使了。 rsync 可以用来清空目录或文件，如下：
>
> ​	1）先建立一个空目录# mkdir/data/blank
>
> ​	2）用rsync删除目标目录
>
> ​	\# rsync --delete-before -d /data/blank/ /var/spool/clientmqueue/
>
> ​	这样目标目录很快就被清空了
>
> ​	注：其中--delete-before 接收者在传输之前进行删除操作

---

### 附录：

### 1、为什么rsync能够快速删除大文件？

​	*1**）**rm命令大量调用了lstat64和unlink，可以推测删除每个文件前都从文件系统中做过一次lstat操作。过程：正式删除工作的第一阶段，需要通过getdirentries64调用，分批读取目录（每次大约为4K），在内存中建立rm的文件列表；第二阶段，lstat64确定所有文件的状态；第三阶段，通过unlink执行实际删除。这三个阶段都有比较多的系统调用和文件系统操作。*

​	*2**）**rsync所做的系统调用很少**：**没有针对单个文件做lstat和unlink操作。命令执行前期，rsync开启了一片共享内存，通过mmap方式加载目录信息。只做目录同步，不需要针对单个文件做unlink。*另外，在其他人的评测里，rm的上下文切换比较多，会造成System CPU占用较多——对于文件系统的操作，简单增加并发数并不总能提升操作速度。 总结：频繁做减法不如直接从头来过把文件系统的目录与书籍的目录做类比，rm删除内容时，将目录的每一个条目逐个删除(unlink)，需要循环重复操作很多次；rsync删除内容时，建立好新的空目录，替换掉老目录，基本没开销。